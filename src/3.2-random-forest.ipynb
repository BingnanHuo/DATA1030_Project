{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Compustat World Firms Industry Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file was created to make parallesim easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Import Libraries and Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform, loguniform\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Preprocessing & Splitting\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, StratifiedGroupKFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modeling\n",
    "from skopt import BayesSearchCV \n",
    "import xgboost\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "# Utils\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustat = pd.read_pickle('../data/compustat_ftreng.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748178, 139)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compustat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_ftrs = ['fyearq','fqtr']\n",
    "num_ftrs = ['saleq', 'gpq', 'oiadpq', 'oibdpq', 'cogsq',\n",
    "       'xoprq', 'atq', 'seqq', 'dlcq', 'dlttq', 'capxy', 'oancfy', 'gpm',\n",
    "       'opm', 'ocfm', 'roa', 'roe', 'cd_ratio', 'ca_ratio', 'fca_ratio',\n",
    "       'fce_ratio', 'fcd_ratio', 'fcs_ratio', 'tat', 'cr', 'tdr', 'der',\n",
    "       'gpm_lag1', 'gpm_lag2', 'gpm_lag3', 'gpm_lag4', 'opm_lag1',\n",
    "       'opm_lag2', 'opm_lag3', 'opm_lag4', 'ocfm_lag1', 'ocfm_lag2',\n",
    "       'ocfm_lag3', 'ocfm_lag4', 'roa_lag1', 'roa_lag2', 'roa_lag3',\n",
    "       'roa_lag4', 'roe_lag1', 'roe_lag2', 'roe_lag3', 'roe_lag4',\n",
    "       'fca_ratio_lag1', 'fca_ratio_lag2', 'fca_ratio_lag3',\n",
    "       'fca_ratio_lag4', 'fce_ratio_lag1', 'fce_ratio_lag2',\n",
    "       'fce_ratio_lag3', 'fce_ratio_lag4', 'fcd_ratio_lag1',\n",
    "       'fcd_ratio_lag2', 'fcd_ratio_lag3', 'fcd_ratio_lag4',\n",
    "       'fcs_ratio_lag1', 'fcs_ratio_lag2', 'fcs_ratio_lag3',\n",
    "       'fcs_ratio_lag4', 'tat_lag1', 'tat_lag2', 'tat_lag3', 'tat_lag4',\n",
    "       'cr_lag1', 'cr_lag2', 'cr_lag3', 'cr_lag4', 'tdr_lag1', 'tdr_lag2',\n",
    "       'tdr_lag3', 'tdr_lag4', 'der_lag1', 'der_lag2', 'der_lag3',\n",
    "       'der_lag4', 'gpm_mean_4Q', 'gpm_std_4Q', 'gpm_mean_8Q',\n",
    "       'gpm_std_8Q', 'opm_mean_4Q', 'opm_std_4Q', 'opm_mean_8Q',\n",
    "       'opm_std_8Q', 'ocfm_mean_4Q', 'ocfm_std_4Q', 'ocfm_mean_8Q',\n",
    "       'ocfm_std_8Q', 'roa_mean_4Q', 'roa_std_4Q', 'roa_mean_8Q',\n",
    "       'roa_std_8Q', 'roe_mean_4Q', 'roe_std_4Q', 'roe_mean_8Q',\n",
    "       'roe_std_8Q', 'fca_ratio_mean_4Q', 'fca_ratio_std_4Q',\n",
    "       'fca_ratio_mean_8Q', 'fca_ratio_std_8Q', 'fce_ratio_mean_4Q',\n",
    "       'fce_ratio_std_4Q', 'fce_ratio_mean_8Q', 'fce_ratio_std_8Q',\n",
    "       'fcd_ratio_mean_4Q', 'fcd_ratio_std_4Q', 'fcd_ratio_mean_8Q',\n",
    "       'fcd_ratio_std_8Q', 'fcs_ratio_mean_4Q', 'fcs_ratio_std_4Q',\n",
    "       'fcs_ratio_mean_8Q', 'fcs_ratio_std_8Q', 'tat_mean_4Q',\n",
    "       'tat_std_4Q', 'tat_mean_8Q', 'tat_std_8Q', 'cr_mean_4Q',\n",
    "       'cr_std_4Q', 'cr_mean_8Q', 'cr_std_8Q', 'tdr_mean_4Q',\n",
    "       'tdr_std_4Q', 'tdr_mean_8Q', 'tdr_std_8Q', 'der_mean_4Q',\n",
    "       'der_std_4Q', 'der_mean_8Q', 'der_std_8Q']\n",
    "\n",
    "#cat_ftrs = ['loc','curcdq']  # maybe not include country for now\n",
    "cat_ftrs = ['curcdq'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler())])\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=True, handle_unknown='ignore'))])\n",
    "preprocessor1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('minmax', minmax_transformer, year_ftrs),\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs)])\n",
    "\n",
    "numeric_transformer2 = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0))])\n",
    "categorical_transformer2 = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))])\n",
    "preprocessor2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('minmax', minmax_transformer, year_ftrs),\n",
    "        ('num', numeric_transformer2, num_ftrs),\n",
    "        ('cat', categorical_transformer2, cat_ftrs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor suitable for XGBoost, will produce missing values\n",
    "\n",
    "Preprocessor2 is for random forest, LR, etc. It will fill missing values with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning w/ RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ML_BayesSearch_CV(X_og, y_og, groups_og, preprocessor, ML_algo, search_space, sample_size=None):\n",
    "    # Loop for 5 random states\n",
    "    # in each loop, split, preprocess, fit, and score\n",
    "    random_states = [377, 575, 610, 777, 233]\n",
    "\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    reports = []\n",
    "    cms = []\n",
    "    best_param = []\n",
    "    best_score = []\n",
    "\n",
    "    for i in range(1):\n",
    "        this_rs = random_states[i]\n",
    "        \n",
    "        # Split into [train,val] and test sets (80%, 20%)\n",
    "        sgss = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=this_rs)\n",
    "        splits = enumerate(sgss.split(X_og, y_og, groups_og))\n",
    "\n",
    "        print('Splitting dataset into train-val and test sets (80%, 20%)')\n",
    "        i, (train_val_idx, test_idx) = next(splits)\n",
    "        X_train_val, X_test = X_og.iloc[train_val_idx], X_og.iloc[test_idx]\n",
    "        y_train_val, y_test = y_og.iloc[train_val_idx], y_og.iloc[test_idx]\n",
    "        group_train_val, group_test = groups_og.iloc[train_val_idx], groups_og.iloc[test_idx]\n",
    "\n",
    "\n",
    "        #full_length = len(y_train_val)\n",
    "        # After getting test set, downsample train-val \n",
    "        if sample_size is None:\n",
    "            pass\n",
    "        elif sample_size <= 0.3*0.8:\n",
    "            print('Sampling dataset for hyperparameter tuning, sample size of train set (80%): ', sample_size*100,'%')\n",
    "            flat_idx = downsample_classes(y_train_val, group_train_val, random_state=this_rs)\n",
    "            flat_X, flat_y, flat_groups = X_train_val.iloc[flat_idx], y_train_val.iloc[flat_idx], group_train_val.iloc[flat_idx]\n",
    "            flat_prop = len(flat_idx)/len(y_train_val)\n",
    "            if int(flat_prop/sample_size)<=1: # no need to do more sampling\n",
    "                X, y = flat_X, flat_y\n",
    "                groups = flat_groups\n",
    "            else:\n",
    "                sampler = StratifiedGroupKFold(n_splits=int(flat_prop/sample_size), shuffle=True, random_state=this_rs)\n",
    "                splits = enumerate(sampler.split(flat_X, flat_y, flat_groups))\n",
    "                i, (_, sampled_idx) = next(splits)\n",
    "                X, y = flat_X.iloc[sampled_idx], flat_y.iloc[sampled_idx]\n",
    "                groups = flat_groups.iloc[sampled_idx]\n",
    "        else:\n",
    "            pass\n",
    "        X_train_val, y_train_val, group_train_val = X, y, groups\n",
    "                \n",
    "\n",
    "        # Split [train,val] into train and val sets (64%, 16%)\n",
    "        sgss_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=this_rs)\n",
    "        pipe = make_pipeline(preprocessor, \n",
    "                             PCA(n_components=0.99, random_state=this_rs),\n",
    "                             ML_algo)\n",
    "\n",
    "        print('Random State Loop: ', i+1)\n",
    "        print('Random State: ', this_rs)\n",
    "        print('***Start Bayes Search***')\n",
    "\n",
    "        bayes_search = BayesSearchCV(estimator=pipe,\n",
    "                                    search_spaces=search_space,\n",
    "                                    n_iter=25,  # Number of iterations\n",
    "                                    cv=sgss_cv,       # Cross-validation strategy\n",
    "                                    n_jobs=-1,  # Use all available cores\n",
    "                                    scoring='f1_weighted',\n",
    "                                    verbose=1,\n",
    "                                    random_state=this_rs)\n",
    "\n",
    "\n",
    "        bayes_search.fit(X_train_val, y_train_val, groups=group_train_val)        \n",
    "    \n",
    "        # Make predictions and calculate accuracy on the test set\n",
    "        y_pred = bayes_search.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(*vote_pred(y_test, y_pred, group_test))\n",
    "        precision = precision_score(*vote_pred(y_test, y_pred, group_test), average=\"weighted\")\n",
    "        recall = recall_score(*vote_pred(y_test, y_pred, group_test), average=\"weighted\")\n",
    "        f1 = f1_score(*vote_pred(y_test, y_pred, group_test), average=\"weighted\")\n",
    "\n",
    "        report = classification_report(*vote_pred(y_test, y_pred, group_test))\n",
    "        cm = confusion_matrix(*vote_pred(y_test, y_pred, group_test))\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        reports.append(report)\n",
    "        cms.append(cm)\n",
    "        best_param.append(bayes_search.best_params_)\n",
    "        best_score.append(bayes_search.best_score_)\n",
    "        \n",
    "    return accuracy_scores, precision_scores, recall_scores, f1_scores, cms, best_param, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748178, 135)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compustat[cat_ftrs] = compustat[cat_ftrs].astype(str)\n",
    "\n",
    "groups = compustat['gvkey']\n",
    "y = compustat['gsector_num']\n",
    "X = compustat.drop(['gvkey','gsector','gsector_num','datafqtr'], axis=1)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train-val and test sets (80%, 20%)\n",
      "Sampling dataset for hyperparameter tuning, sample size of train set (80%):  4.0 %\n",
      "Random State Loop:  1\n",
      "Random State:  377\n",
      "***Start Bayes Search***\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    }
   ],
   "source": [
    "search_space_rf = {\n",
    "    'randomforestclassifier__n_estimators': Integer(100, 300),\n",
    "    'randomforestclassifier__max_features': Categorical(['sqrt', 'log2']),\n",
    "    'randomforestclassifier__max_depth': Integer(3, 35),\n",
    "    'randomforestclassifier__min_samples_split': Integer(3, 50)\n",
    "}\n",
    "rf_clf = RandomForestClassifier()\n",
    "acc_rf, pre_rf, rec_rf, f1_rf, \\\n",
    "    cm_rf, params_rf, bs_rf = ML_BayesSearch_CV(X, y, groups, preprocessor2,  \n",
    "                                                rf_clf, search_space_rf, sample_size=0.04)\n",
    "rf_results = {\n",
    "    \"accuracy\": acc_rf,\n",
    "    \"precision\": pre_rf,\n",
    "    \"recall\": rec_rf,\n",
    "    \"f1_score\": f1_rf,\n",
    "    \"conf_matrix\": cm_rf,\n",
    "    \"params\": params_rf,\n",
    "    \"best_score\": bs_rf\n",
    "}\n",
    "with open('../results/rf_results.pkl', 'wb') as file:\n",
    "    pickle.dump(rf_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.3866120218579235, 0.32432432432432434],\n",
       " 'precision': [0.39653280324682, 0.4835263835263836],\n",
       " 'recall': [0.3866120218579235, 0.32432432432432434],\n",
       " 'f1_score': [0.38006405679977817, 0.2975975975975976],\n",
       " 'conf_matrix': [array([[32,  4,  6,  2,  4,  5,  1,  1,  1,  4,  4],\n",
       "         [12,  8,  8,  4,  6,  4,  0,  6,  1,  5,  1],\n",
       "         [ 0,  1, 21,  2, 10,  5,  0, 10,  4,  3,  3],\n",
       "         [ 1,  6, 10,  6, 11,  6,  0, 12,  5,  2, 10],\n",
       "         [ 2,  5,  6,  5,  9,  4,  0,  6,  4,  4,  4],\n",
       "         [ 4,  2,  3,  4,  9, 29,  0, 12,  4,  1,  2],\n",
       "         [ 6,  1,  2,  1,  1,  7, 48,  3,  1,  0, 11],\n",
       "         [ 7,  4, 14,  4,  3,  5,  1, 37,  6,  1,  3],\n",
       "         [ 3,  1,  7,  2,  4,  7,  4, 16, 19,  6,  9],\n",
       "         [ 4,  1,  8,  2,  5,  1,  0,  3,  1, 28,  5],\n",
       "         [ 3,  1,  2,  1,  0,  4,  2,  3,  1,  1, 46]]),\n",
       "  array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 1, 0, 1, 1, 2, 0, 0],\n",
       "         [0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 1, 0, 1, 0, 2, 3, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]])],\n",
       " 'params': [OrderedDict([('randomforestclassifier__max_depth', 31),\n",
       "               ('randomforestclassifier__max_features', 'sqrt'),\n",
       "               ('randomforestclassifier__min_samples_split', 2),\n",
       "               ('randomforestclassifier__n_estimators', 300)]),\n",
       "  OrderedDict([('randomforestclassifier__max_depth', 34),\n",
       "               ('randomforestclassifier__max_features', 'log2'),\n",
       "               ('randomforestclassifier__min_samples_split', 9),\n",
       "               ('randomforestclassifier__n_estimators', 275)])],\n",
       " 'best_score': [0.3129313690555942, 0.2689507890321006]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results_old = pickle.load(open('../results/rf_results.pkl', 'rb'))\n",
    "rf_results_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33799999999999997"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.296,0.38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.29651310766098243],\n",
       " 'precision': [0.34258457837687156],\n",
       " 'recall': [0.29651310766098243],\n",
       " 'f1_score': [0.2958998714402831],\n",
       " 'conf_matrix': [array([[262,  47,   9,  34,  60,  66,  13,  32,  55,  73,  24],\n",
       "         [157, 412,  69, 165, 238, 140,   8,  95,  77,  99,  38],\n",
       "         [ 92, 186, 200, 278, 467, 136,  26, 300, 177, 180, 106],\n",
       "         [ 67, 157,  85, 359, 327, 145,  31, 222, 211,  89,  94],\n",
       "         [ 31,  60,  44, 126, 215,  73,   4,  84,  92,  45,  31],\n",
       "         [ 60,  55,  41,  56, 117, 578,  18, 169, 116,  26,  18],\n",
       "         [ 33,  17,   6,  24,  19,  38, 430,  18,  44,  19,  97],\n",
       "         [ 69,  94, 106, 170, 245, 283,  11, 623, 194,  45,  37],\n",
       "         [ 34,  16,  21,  51,  40,  73,   6,  87, 152,  42,  23],\n",
       "         [ 22,   9,   6,  10,  19,  12,   7,  11,  12, 173,  16],\n",
       "         [ 13,   6,   2,   7,   7,   5,   3,   4,   3,  15,  91]])],\n",
       " 'params': [OrderedDict([('randomforestclassifier__max_depth', 29),\n",
       "               ('randomforestclassifier__max_features', 'sqrt'),\n",
       "               ('randomforestclassifier__min_samples_split', 6),\n",
       "               ('randomforestclassifier__n_estimators', 297)])],\n",
       " 'best_score': [0.3123458239900814]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = {\n",
    "    \"accuracy\": acc_rf,\n",
    "    \"precision\": pre_rf,\n",
    "    \"recall\": rec_rf,\n",
    "    \"f1_score\": f1_rf,\n",
    "    \"conf_matrix\": cm_rf,\n",
    "    \"params\": params_rf,\n",
    "    \"best_score\": bs_rf\n",
    "}\n",
    "with open('../results/rf_results.pkl', 'wb') as file:\n",
    "    pickle.dump(rf_results, file)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
